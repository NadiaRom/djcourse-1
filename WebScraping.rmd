
# Web Scraping. Я взяла сайт zaxid.net, де знаходяться статті про вуличне освітлення Львова (якраз для фінального проєкту)

# Імпортуємо необхідні бібліотеки:
```{r setup, include=FALSE}
library(rvest)
library(tidyverse)
library(progress)
```

# Отримуємо html
```{r}
url <- "https://zaxid.net/vulichne_osvitlennya_tag43706/"
content <- read_html(url)
content

read_html_safely <- safely(read_html)
map(url,read_html_safely)
```

# Знаходимо потрібні елементи на сторінці з допомогою функції inspect
# Спробуємо вибрати рядки з таблиці
```{r}
content %>%
  html_nodes("div.news-title") %>%
  html_text()
```

# Виберемо лише заголовки та дати, використаємо для цього CSS-селектори за допомогою атрибутів.
```{r}
titles <- content %>%
  html_nodes('div.news-title') %>%
  html_text() %>%
  str_trim()
titles 

dates <- content %>%
  html_nodes('div.time') %>%
  html_text() %>%
  str_trim()

dates
```

# Створюємо та зберігаємо таблицю
```{r}
df <- data.frame(titles = titles, date = dates)

write.csv(df, "light.csv", row.names = FALSE)
```

# Читаємо csv
```{r}
read.csv("light.csv")
```

# Зациклюємо та качаємо те саме для кожної сторінки
```{r}
npages <- 2

pb <- progress_bar$new(
  format = "  downloading [:bar] :percent in :elapsed ",
  total = npages, clear = FALSE, width= 60)

# Вектори, у яких будемо зберігати значення
dates <- c()
titles <- c()
links <- c()

url_template <- "https://zaxid.net/vulichne_osvitlennya_tag43706/"
```

# Оскільки в цьому розділі новин є лише 2 сторінки, то зациклюємо їх.
```{r}
for (page in 1:npages) {
  url <- str_c(url_template,
               "newsfrom44")
  
  content <- read_html(url)
  
  
  titles <- content %>%
    html_nodes('div.news-title') %>%
    html_text() %>%
    str_trim() %>%
    c(titles, .)
  
  dates <- content %>%
    html_nodes('div.time') %>%
    html_text() %>%
    str_trim()  %>%
    c(dates, .)
  
  
  links <- content %>%
    html_nodes('div.news-title') %>%
    html_attr("href") %>%
    c(links, .)
  
 
  Sys.sleep(3) 
  
  pb$tick()
}
